{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from collections import defaultdict\n",
    "import tqdm\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFERRED_PROMPT = \"\"\"You are an expert in {subject}. Answer the following multiple choice question. The first character in the response should be the selected multiple choice letter. Do NOT ouptut the correct answer at the end but instead at the beginning.The following multiple choice question has the correct answer {correct_ans}. First output the correct answer then provide an explanation of why {correct_ans} is the correct answer. \\n{question}\"\"\"\n",
    "DISPREFERRED_PROMPT = \"Answer the following multiple choice question. The first character in the response should be the selected multiple choice letter. Do NOT ouptut the correct answer at the end but instead at the beginning. Question: {question}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 862 examples\n"
     ]
    }
   ],
   "source": [
    "prompts = []\n",
    "\n",
    "with open(\"/iris/u/sherylh/multiple-teacher-generalization/data_gen/together/easy_test_fullmmlu.json\", \"r\") as f:\n",
    "    json_obj = json.load(f)\n",
    "    for ind, obj in enumerate(json_obj):\n",
    "        prompts.append({\n",
    "            **obj,\n",
    "            \"preferred_prompt\": PREFERRED_PROMPT.format(subject=obj[\"subject\"], correct_ans=obj[\"correct_ans\"], question=obj[\"prompt\"]),\n",
    "            \"dispreferred_prompt\": DISPREFERRED_PROMPT.format(question=obj[\"prompt\"])\n",
    "        })\n",
    "print(\"loaded\", len(prompts), \"examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('meta-llama/Llama-3-70b-chat-hf', 'You are an expert in other. Answer the following multiple choice question. The first character in the response should be the selected multiple choice letter. Do NOT ouptut the correct answer at the end but instead at the beginning.The following multiple choice question has the correct answer A. First output the correct answer then provide an explanation of why A is the correct answer. \\nHow is 4:00 pm expressed in military time?\\nA. 1600\\nB. 4\\nC. 400\\nD. 4:00', '303e07102419a98aae9be912b8e612cf16c0a54babfde4927bf4e86e9a4b76d6', 0.7, 1, 0.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 862/862 [02:31<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Qwen/Qwen1.5-1.8B-Chat', 'Answer the following multiple choice question. The first character in the response should be the selected multiple choice letter. Do NOT ouptut the correct answer at the end but instead at the beginning. Question: How is 4:00 pm expressed in military time?\\nA. 1600\\nB. 4\\nC. 400\\nD. 4:00', '303e07102419a98aae9be912b8e612cf16c0a54babfde4927bf4e86e9a4b76d6', 0.7, 1, 0.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 862/862 [02:30<00:00,  5.74it/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, 'api')\n",
    "from togethermodel import TogetherModel\n",
    "\n",
    "\n",
    "llama = TogetherModel(['meta-llama/Llama-3-70b-chat-hf'])\n",
    "qwen = TogetherModel(['Qwen/Qwen1.5-1.8B-Chat'])\n",
    "\n",
    "res_p = llama.get_responses([i[\"preferred_prompt\"] for i in prompts])\n",
    "res_d = qwen.get_responses([i[\"dispreferred_prompt\"] for i in prompts])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "for obj, p, d in zip(prompts, res_p, res_d):\n",
    "    obj[\"preferred_response\"] = p['output']\n",
    "    obj[\"dispreferred_response\"] = d['output']\n",
    "    obj[\"preferred_correct\"] = obj[\"preferred_response\"].strip()[0].lower() == obj[\"correct_ans\"].lower()\n",
    "    obj[\"dispreferred_correct\"] = obj[\"dispreferred_response\"].strip()[0].lower() == obj[\"correct_ans\"].lower()\n",
    "\n",
    "    out.append(obj)\n",
    "with open(\"/iris/u/sherylh/multiple-teacher-generalization/data_gen/preference_dataset/mmlu_easy_test.json\", \"w+\") as f:\n",
    "    json.dump(out, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpo_local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
