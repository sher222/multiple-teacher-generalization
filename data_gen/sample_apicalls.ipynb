{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from collections import defaultdict\n",
    "import tqdm\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = list(data.keys())\n",
    "random.shuffle(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"Please evaluate the following prompt and rate on a scale of 1 to 5 based on how reasonable of a prompt it is for a human to ask an LLM. The prompt should have a clear question or request and also be a prompt an actual human is likely to ask a LLM. For example, a prompt like \"hello\" or \"you're stupid\" is a 1 while a prompt like \"What are five ways to prepare for a coding interview\" or \"Explain the concept of acceleration to me\" would be a 5. Output a sentence long explanation followed by the score. The score should be the last thing outputted, so the output takes the format of <Explanation> Score: <number from 1 to 5>. Prompt: {prompt}\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOGETHER_API_KEY\"] = \"da4a68310cb6eb918e9f34d9509695bbccba1585555875aa384ad71fa9d15a84\"\n",
    "import sys\n",
    "sys.path.insert(1, '../api')\n",
    "from togetherclass import TogetherModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7790/7790 [04:12<00:00, 30.84it/s] \n"
     ]
    }
   ],
   "source": [
    "queries = [[PROMPT.format(prompt=i)] for i in prompts]\n",
    "resp = TogetherModel(\"meta-llama/Llama-3-70b-chat-hf\").get_responses(queries, {\"temperature\": 0.6, \"top_p\": 0.9})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleared_prompts = []\n",
    "with open(\"../data/prompts_gpt.jsonl\", \"w+\") as f:\n",
    "    for i, j in list(zip(prompts, resp)):\n",
    "        try:\n",
    "            score = int(j.split(\"Score:\")[-1].replace(\".\",\"\"))\n",
    "        except ValueError:\n",
    "            score = -1\n",
    "        if score >= 4:\n",
    "            cleared_prompts.append(i)\n",
    "        f.write(json.dumps({\n",
    "            \"prompt\": i,\n",
    "            \"resp\": j,\n",
    "            \"score\": score\n",
    "        })+ \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/iris/u/sherylh/latent-dpo/latent-dpo/data/test_apicleaned.jsonl\", \"w+\") as f:\n",
    "    for i in test_prompts:\n",
    "        for l in data[i]:\n",
    "            f.write(json.dumps(l)+\"\\n\")\n",
    "with open(\"/iris/u/sherylh/latent-dpo/latent-dpo/data/train_apicleaned.jsonl\", \"w+\") as f:\n",
    "    for i in train_prompts:\n",
    "        for l in data[i]:\n",
    "            f.write(json.dumps(l)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpo_local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
